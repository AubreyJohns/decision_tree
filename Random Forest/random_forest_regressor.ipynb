{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor\n",
    "\n",
    "Random forest method achieves better performance than a single decision tree simply by averaging the predictions of many decision trees.\n",
    "\n",
    "We refer to the random forest method as an **ensemble method**. By definition, ensemble methods combine the predictions of several models (e.g., several trees, in the case of random forests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load boston housing csv\n",
    "boston_housing_df = pd.read_csv('boston_housing.csv')\n",
    "boston_housing_df.head()                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    208500\n",
      "1    181500\n",
      "2    223500\n",
      "3    140000\n",
      "4    250000\n",
      "Name: SalePrice, dtype: int64\n",
      "   LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n",
      "0     8450       2003       856       854         2             3   \n",
      "1     9600       1976      1262         0         2             3   \n",
      "2    11250       2001       920       866         2             3   \n",
      "3     9550       1915       961       756         1             3   \n",
      "4    14260       2000      1145      1053         2             4   \n",
      "\n",
      "   TotRmsAbvGrd  \n",
      "0             8  \n",
      "1             6  \n",
      "2             6  \n",
      "3             7  \n",
      "4             9  \n"
     ]
    }
   ],
   "source": [
    "# define dependent and independent variables\n",
    "y = boston_housing_df['SalePrice']\n",
    "print(y.head())\n",
    "predictive_features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "x = boston_housing_df[predictive_features]\n",
    "print(x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data's x\n",
      "      LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n",
      "6       10084       2004      1694         0         2             3   \n",
      "807     21384       1923      1072       504         1             3   \n",
      "955      7136       1946       979       979         2             4   \n",
      "1040    13125       1957      1803         0         2             3   \n",
      "701      9600       1969      1164         0         1             3   \n",
      "\n",
      "      TotRmsAbvGrd  \n",
      "6                7  \n",
      "807              6  \n",
      "955              8  \n",
      "1040             8  \n",
      "701              6  \n",
      "Training data's y\n",
      "6       307000\n",
      "807     223500\n",
      "955     145000\n",
      "1040    155000\n",
      "701     140000\n",
      "Name: SalePrice, dtype: int64\n",
      "Testing data's x\n",
      "      LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n",
      "258     12435       2001       963       829         2             3   \n",
      "267      8400       1939      1052       720         2             4   \n",
      "288      9819       1967       900         0         1             3   \n",
      "649      1936       1970       630         0         1             1   \n",
      "1233    12160       1959      1188         0         1             3   \n",
      "\n",
      "      TotRmsAbvGrd  \n",
      "258              7  \n",
      "267              8  \n",
      "288              5  \n",
      "649              3  \n",
      "1233             6  \n",
      "Testing data's y\n",
      "258     231500\n",
      "267     179500\n",
      "288     122000\n",
      "649      84500\n",
      "1233    142000\n",
      "Name: SalePrice, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(x, y, random_state=1)\n",
    "print(\"Training data's x\")\n",
    "print(train_x.head())\n",
    "print(\"Training data's y\")\n",
    "print(train_y.head())\n",
    "print(\"Testing data's x\")\n",
    "print(test_x.head())\n",
    "print(\"Testing data's y\")\n",
    "print(test_y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mae based on training data: 21857.15912981083\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "boston_housing_model = RandomForestRegressor(random_state=1)\n",
    "\n",
    "# fit the model using train data\n",
    "boston_housing_model.fit(train_x, train_y)\n",
    "\n",
    "# make predictions on test data\n",
    "predicted_prices = boston_housing_model.predict(test_x)\n",
    "\n",
    "# evaluate model\n",
    "mae = mean_absolute_error(test_y, predicted_prices)\n",
    "print('Mae based on training data:', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model using all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mae based on all data: 8299.72804109589\n"
     ]
    }
   ],
   "source": [
    "# fit the model using all data\n",
    "boston_housing_model.fit(x, y)\n",
    "\n",
    "# make predictions on all data\n",
    "predicted_prices = boston_housing_model.predict(x)\n",
    "\n",
    "# evaluate model\n",
    "mae = mean_absolute_error(y, predicted_prices)\n",
    "print('Mae based on all data:',mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations made:\n",
    "    \n",
    "* The model performs better when all the data is used in training (8299.72804109589 < 21857.15912981083).\n",
    "* Random Forest performs better than Decision Tree Regressor (8299.72804109589 < 16629.182745225255)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating several models\n",
    "\n",
    "We set the number of trees in the random forest model with the **n_estimators** parameter, and setting **random_state** ensures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
    "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
    "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
    "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4, model_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 MAE: 22285\n",
      "Model 2 MAE: 22118\n",
      "Model 3 MAE: 22152\n",
      "Model 4 MAE: 22624\n",
      "Model 5 MAE: 22401\n"
     ]
    }
   ],
   "source": [
    "# Function for comparing different models\n",
    "def score_model(model, X_t=train_x, X_v=test_x, y_t=train_y, y_v=test_y):\n",
    "    model.fit(X_t, y_t)\n",
    "    preds = model.predict(X_v)\n",
    "    return mean_absolute_error(y_v, preds)\n",
    "\n",
    "for i in range(0, len(models)):\n",
    "    mae = score_model(models[i])\n",
    "    print(\"Model %d MAE: %d\" % (i+1, mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 is the best since it has the lowest MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Missing values\n",
    "Most machine learning libraries (including scikit-learn) give an error if you try to build a model using data with missing values. So you'll need to choose one of the strategies below:\n",
    "* A Simple Option: Drop Columns with Missing Values\n",
    "* A Better Option: Imputation\n",
    "* An Extension To Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 0\n",
       "MSSubClass         0\n",
       "MSZoning           0\n",
       "LotFrontage      259\n",
       "LotArea            0\n",
       "                ... \n",
       "MoSold             0\n",
       "YrSold             0\n",
       "SaleType           0\n",
       "SaleCondition      0\n",
       "SalePrice          0\n",
       "Length: 81, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_housing_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score from Approach 1 (Drop Columns with Missing Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = boston_housing_df['SalePrice']\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "boston_housing_df_missing_saleprice = boston_housing_df.drop(['SalePrice'], axis=1)\n",
    "X = boston_housing_df_missing_saleprice.select_dtypes(exclude=['object'])\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Function to Measure Quality of Each Approach\n",
    "\n",
    "We define a function score_dataset() to compare different approaches to dealing with missing values. This function reports the mean absolute error (MAE) from a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with both training and validation sets, we are careful to drop the same columns in both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop columns with missing values):\n",
      "19003.3198630137\n"
     ]
    }
   ],
   "source": [
    "# Drop columns in training and validation data\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score from Approach 2 (Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Imputation):\n",
      "19243.57294520548\n"
     ]
    }
   ],
   "source": [
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print(\"MAE from Approach 2 (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score from Approach 3 (An Extension to Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (An Extension to Imputation):\n",
      "19349.617465753425\n"
     ]
    }
   ],
   "source": [
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"MAE from Approach 3 (An Extension to Imputation):\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kaggle's Missing Values](https://www.kaggle.com/alexisbcook/missing-values)\n",
    "\n",
    "[Kaggle's Missing Values Exercise](https://www.kaggle.com/aubreyjohn/exercise-missing-values/edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that thre are so few missing values in the dataset, we'd expect imputation to perform better than dropping columns entirely. However, we see that dropping columns performs slightly better! \n",
    "\n",
    "While this can probably partially be attributed to noise in the dataset, another potential explanation is that the imputation method is not a great match to this dataset. \n",
    "\n",
    "That is, maybe instead of filling in the mean value, it makes more sense to set every missing value to a value of 0, to fill in the most frequently encountered value, or to use some other method. \n",
    "\n",
    "For instance, consider the GarageYrBlt column (which indicates the year that the garage was built). It's likely that in some cases, a missing value could indicate a house that does not have a garage. \n",
    "\n",
    "Does it make more sense to fill in the median value along each column in this case? Or could we get better results by filling in the minimum value along each column? \n",
    "\n",
    "It's not quite clear what's best in this case, but perhaps we can rule out some options immediately - for instance, setting missing values in this column to 0 is likely to yield horrible results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables\n",
    "A categorical variable takes only a limited number of values.\n",
    "\n",
    "You will get an error if you try to plug these variables into most machine learning models in Python without preprocessing them first. \n",
    "\n",
    "We'll compare three approaches that you can use to prepare your categorical data:\n",
    "* Drop Categorical VariablesÂ¶\n",
    "* Label Encoding\n",
    "* One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aubrey/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "# Separate target from predictors\n",
    "y = boston_housing_df.SalePrice\n",
    "X = boston_housing_df.drop(['SalePrice'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# Drop columns with missing values\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>Condition2</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>...</td>\n",
       "      <td>774</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>PosN</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>...</td>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>...</td>\n",
       "      <td>432</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>CulDSac</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>...</td>\n",
       "      <td>857</td>\n",
       "      <td>150</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>...</td>\n",
       "      <td>843</td>\n",
       "      <td>468</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MSZoning Street LotShape LandContour Utilities LotConfig LandSlope  \\\n",
       "618       RL   Pave      Reg         Lvl    AllPub    Inside       Gtl   \n",
       "870       RL   Pave      Reg         Lvl    AllPub    Inside       Gtl   \n",
       "92        RL   Pave      IR1         HLS    AllPub    Inside       Gtl   \n",
       "817       RL   Pave      IR1         Lvl    AllPub   CulDSac       Gtl   \n",
       "302       RL   Pave      IR1         Lvl    AllPub    Corner       Gtl   \n",
       "\n",
       "    Condition1 Condition2 BldgType  ... GarageArea WoodDeckSF OpenPorchSF  \\\n",
       "618       Norm       Norm     1Fam  ...        774          0         108   \n",
       "870       PosN       Norm     1Fam  ...        308          0           0   \n",
       "92        Norm       Norm     1Fam  ...        432          0           0   \n",
       "817       Norm       Norm     1Fam  ...        857        150          59   \n",
       "302       Norm       Norm     1Fam  ...        843        468          81   \n",
       "\n",
       "    EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold  \n",
       "618             0         0         260        0       0      7   2007  \n",
       "870             0         0           0        0       0      8   2009  \n",
       "92             44         0           0        0       0      8   2009  \n",
       "817             0         0           0        0       0      7   2008  \n",
       "302             0         0           0        0       0      1   2006  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we obtain a list of all of the categorical variables in the training data.\n",
    "\n",
    "We do this by checking the data type (or dtype) of each column. The object dtype indicates a column has text (there are other things it could theoretically be, but that's unimportant for our purposes). \n",
    "\n",
    "For this dataset, the columns with text indicate categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function score_dataset() to compare the three different approaches to dealing with categorical variables. \n",
    "\n",
    "This function reports the mean absolute error (MAE) from a random forest model. I\n",
    "\n",
    "n general, we want the MAE to be as low as possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Categorical VariablesÂ¶\n",
    "The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.\n",
    "\n",
    "We drop the object columns with the select_dtypes() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop categorical variables):\n",
      "17952.591404109586\n"
     ]
    }
   ],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "Label encoding assigns each unique value to a different integer.\n",
    "\n",
    "![Label encoding](https://i.imgur.com/tEogUAr.png)\n",
    "\n",
    "This approach assumes an ordering of the categories: \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3).\n",
    "\n",
    "This assumption makes sense in this example, because there is an indisputable ranking to the categories. \n",
    "\n",
    "Not all categorical variables have a clear ordering in the values, but we refer to those that do as **ordinal variables**. \n",
    "\n",
    "For tree-based models (like decision trees and random forests), you can expect label encoding to work well with ordinal variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a label encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. \n",
    "\n",
    "In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them.\n",
    "\n",
    "The simplest approach, however, is to drop the problematic categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns that will be label encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'LotConfig', 'BldgType', 'HouseStyle', 'ExterQual', 'CentralAir', 'KitchenQual', 'PavedDrive', 'SaleCondition']\n",
      "\n",
      "Categorical columns that will be dropped from the dataset: ['Condition2', 'Heating', 'RoofStyle', 'ExterCond', 'RoofMatl', 'Functional', 'Condition1', 'Foundation', 'SaleType', 'LandSlope', 'Utilities', 'HeatingQC']\n"
     ]
    }
   ],
   "source": [
    "# All categorical columns\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely label encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(X_train[col]) == set(X_valid[col])]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be label encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Label Encoding):\n",
      "17622.9305479452\n"
     ]
    }
   ],
   "source": [
    "# Drop categorical columns that will not be encoded\n",
    "label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
    "label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
    "\n",
    "# Apply label encoder to each column with categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "for col in good_label_cols:\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid[col])\n",
    "\n",
    "print(\"MAE from Approach 2 (Label Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data.\n",
    "\n",
    "![One Hot encoding](https://i.imgur.com/TW5m0aJ.png)\n",
    "\n",
    "In contrast to label encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., \"Red\" is neither more nor less than \"Yellow\"). \n",
    "\n",
    "We refer to categorical variables without an intrinsic ranking as **nominal variables**.\n",
    "\n",
    "One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values).\n",
    "\n",
    "**N/B**\n",
    "\n",
    "For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset.  For this reason, we typically will only one-hot encode columns with relatively low cardinality.  Then, high cardinality columns can either be dropped from the dataset, or we can use label encoding.\n",
    "\n",
    "As an example, consider a dataset with 10,000 rows, and containing one categorical column with 100 unique entries.  \n",
    "- If this column is replaced with the corresponding one-hot encoding, how many entries are added to the dataset?  \n",
    "- If we instead replace the column with the label encoding, how many entries are added?  \n",
    "\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "To calculate how many entries are added to the dataset through the one-hot encoding, begin by calculating how many entries are needed to encode the categorical variable (by multiplying the number of rows by the number of columns in the one-hot encoding). Then, to obtain how many entries are added to the dataset, subtract the number of entries in the original column.\n",
    "\n",
    "990000\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (One-Hot Encoding):\n",
      "17514.224246575344\n"
     ]
    }
   ],
   "source": [
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, one-hot encoding (Approach 3) will typically perform best, and dropping the categorical columns (Approach 1) typically performs worst, but it varies on a case-by-case basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "Pipelines are a simple way to keep your data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.\n",
    "\n",
    "Many data scientists hack together models without pipelines, but pipelines have some important benefits. Those include:\n",
    "\n",
    "1. Cleaner Code: Accounting for data at each step of preprocessing can get messy. With a pipeline, you won't need to manually keep track of your training and validation data at each step.\n",
    "2. Fewer Bugs: There are fewer opportunities to misapply a step or forget a preprocessing step.\n",
    "3. Easier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.\n",
    "4. More Options for Model Validation: You will see an example in the next tutorial, which covers cross-validation.\n",
    "\n",
    "We construct the full pipeline in three steps:\n",
    "1.  Define Preprocessing Steps\n",
    "2. Define the Model\n",
    "3. Create and Evaluate the Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read dataset\n",
    "boston_housing_df = pd.read_csv('boston_housing.csv')\n",
    "boston_housing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define target and predictive variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
      "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC Fence MiscFeature  \\\n",
      "0         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "1         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "2         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "3         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "4         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "\n",
      "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
      "0       0      2    2008        WD         Normal  \n",
      "1       0      5    2007        WD         Normal  \n",
      "2       0      9    2008        WD         Normal  \n",
      "3       0      2    2006        WD        Abnorml  \n",
      "4       0     12    2008        WD         Normal  \n",
      "\n",
      "[5 rows x 80 columns]\n",
      "0    208500\n",
      "1    181500\n",
      "2    223500\n",
      "3    140000\n",
      "4    250000\n",
      "Name: SalePrice, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def define_target_predictive_variables(data, target):\n",
    "    '''\n",
    "    takes 2 parameters: data(dataframe with all data) and target(column to act as target)\n",
    "    returns target(y) and predictive variables(x)\n",
    "    '''\n",
    "    # check if target columns is missing any value\n",
    "    if data[target].isnull().any():\n",
    "        # remove rows with missing target and separate from predictors\n",
    "        x_full = data.dropna(axis=0, subset=[target]) #, inplace=True\n",
    "        y = x_full[target]\n",
    "        x = x_full.drop([target], axis=1) #, inplace=True\n",
    "    else:\n",
    "        y = data[target]\n",
    "        x = data.drop([target], axis=1) #, inplace=True\n",
    "        \n",
    "    return (x, y)\n",
    "\n",
    "    \n",
    "x, y = define_target_predictive_variables(boston_housing_df, 'SalePrice')\n",
    "\n",
    "print(x.head())\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split x and y into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split x and y into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get rid of low cardinality columms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    MSZoning Street Alley LotShape LandContour Utilities LotConfig LandSlope  \\\n",
      "618       RL   Pave   NaN      Reg         Lvl    AllPub    Inside       Gtl   \n",
      "870       RL   Pave   NaN      Reg         Lvl    AllPub    Inside       Gtl   \n",
      "92        RL   Pave  Grvl      IR1         HLS    AllPub    Inside       Gtl   \n",
      "817       RL   Pave   NaN      IR1         Lvl    AllPub   CulDSac       Gtl   \n",
      "302       RL   Pave   NaN      IR1         Lvl    AllPub    Corner       Gtl   \n",
      "\n",
      "    Condition1 Condition2  ... GarageArea WoodDeckSF OpenPorchSF  \\\n",
      "618       Norm       Norm  ...        774          0         108   \n",
      "870       PosN       Norm  ...        308          0           0   \n",
      "92        Norm       Norm  ...        432          0           0   \n",
      "817       Norm       Norm  ...        857        150          59   \n",
      "302       Norm       Norm  ...        843        468          81   \n",
      "\n",
      "    EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold  \n",
      "618             0         0         260        0       0      7   2007  \n",
      "870             0         0           0        0       0      8   2009  \n",
      "92             44         0           0        0       0      8   2009  \n",
      "817             0         0           0        0       0      7   2008  \n",
      "302             0         0           0        0       0      1   2006  \n",
      "\n",
      "[5 rows x 77 columns]\n",
      "    MSZoning Street Alley LotShape LandContour Utilities LotConfig LandSlope  \\\n",
      "529       RL   Pave   NaN      IR1         Lvl    AllPub   CulDSac       Gtl   \n",
      "491       RL   Pave   NaN      Reg         Lvl    AllPub    Inside       Gtl   \n",
      "459       RL   Pave   NaN      IR1         Bnk    AllPub    Corner       Gtl   \n",
      "279       RL   Pave   NaN      Reg         Lvl    AllPub    Inside       Gtl   \n",
      "655       RM   Pave   NaN      Reg         Lvl    AllPub    Inside       Gtl   \n",
      "\n",
      "    Condition1 Condition2  ... GarageArea WoodDeckSF OpenPorchSF  \\\n",
      "529       Norm       Norm  ...        484          0           0   \n",
      "491     Artery       Norm  ...        240          0           0   \n",
      "459       Norm       Norm  ...        352          0           0   \n",
      "279       Norm       Norm  ...        505        288         117   \n",
      "655       Norm       Norm  ...        264          0           0   \n",
      "\n",
      "    EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold  \n",
      "529           200         0           0        0       0      3   2007  \n",
      "491            32         0           0        0       0      8   2006  \n",
      "459           248         0           0        0       0      7   2009  \n",
      "279             0         0           0        0       0      3   2008  \n",
      "655             0         0           0        0       0      3   2010  \n",
      "\n",
      "[5 rows x 77 columns]\n"
     ]
    }
   ],
   "source": [
    "def remove_low_cardinality_cols(x_train, x_test):\n",
    "    # select categorical columns with relatively low cardinality\n",
    "    categorical_cols = [col for col in x_train.columns\n",
    "                        if x_train[col].nunique() < 10\n",
    "                       and x_train[col].dtype == 'object']\n",
    "\n",
    "    numerical_cols = [col for col in x_train.columns\n",
    "                        if x_train[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    # keep selected columns only\n",
    "    selected_cols = categorical_cols + numerical_cols\n",
    "    x_train = x_train[selected_cols].copy()\n",
    "    x_test = x_test[selected_cols].copy()\n",
    "    \n",
    "    return (categorical_cols, numerical_cols, x_train, x_test)\n",
    "\n",
    "categorical_cols, numerical_cols, x_train, x_test = remove_low_cardinality_cols(x_train, x_test)\n",
    "print(x_train.head())\n",
    "print(x_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17721.08506849315\n"
     ]
    }
   ],
   "source": [
    "# preprocessing numerical columns\n",
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# preprocessing categorical columns\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# bundle preprocessing of numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "# define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# create the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# preprocessing of training data and fit model\n",
    "pipeline = pipeline.fit(x_train, y_train)\n",
    "\n",
    "# preprocessing of testing data and get predictions\n",
    "preds = pipeline.predict(x_test)\n",
    "\n",
    "# evaluate model\n",
    "score = mean_absolute_error(y_test, preds)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is an iterative process.\n",
    "\n",
    "You will face choices about what predictive variables to use, what types of models to use, what arguments to supply to those models, etc. So far, you have made these choices in a data-driven way by measuring model quality with a validation (or holdout) set.\n",
    "\n",
    "But there are some drawbacks to this approach. To see this, imagine you have a dataset with 5000 rows. You will typically keep about 20% of the data as a validation dataset, or 1000 rows. But this leaves some random chance in determining model scores. That is, a model might do well on one set of 1000 rows, even if it would be inaccurate on a different 1000 rows.\n",
    "\n",
    "At an extreme, you could imagine having only 1 row of data in the validation set. If you compare alternative models, which one makes the best predictions on a single data point will be mostly a matter of luck!\n",
    "\n",
    "In general, the larger the validation set, the less randomness (aka \"noise\") there is in our measure of model quality, and the more reliable it will be. Unfortunately, we can only get a large validation set by removing rows from our training data, and smaller training datasets mean worse models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "In cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality.\n",
    "\n",
    "For example, we could begin by dividing the data into 5 pieces, each 20% of the full dataset. In this case, we say that we have broken the data into 5 \"folds\".\n",
    "\n",
    "![Cross Validation](https://i.imgur.com/9k60cVA.png)\n",
    "\n",
    "Then, we run one experiment for each fold:\n",
    "\n",
    "- In Experiment 1, we use the first fold as a validation (or holdout) set and everything else as training data. This gives us a measure of model quality based on a 20% holdout set.\n",
    "- In Experiment 2, we hold out data from the second fold (and use everything except the second fold for training the model). The holdout set is then used to get a second estimate of model quality.\n",
    "- We repeat this process, using every fold once as the holdout set. Putting this together, 100% of the data is used as holdout at some point, and we end up with a measure of model quality that is based on all of the rows in the dataset (even if we don't use all rows simultaneously).\n",
    "\n",
    "\n",
    "\n",
    "### Advantages\n",
    "Cross-validation gives a more accurate measure of model quality, which is especially important if you are making a lot of modeling decisions. \n",
    "\n",
    "### Drawbacks\n",
    "However, it can take longer to run, because it estimates multiple models (one for each fold).\n",
    "\n",
    "So, given these tradeoffs, \n",
    "\n",
    "### When should you use cross-validation?\n",
    "- For small datasets, where extra computational burden isn't a big deal, you should run cross-validation.\n",
    "- For larger datasets, a single validation set is sufficient. Your code will run faster, and you may have enough data that there's little need to re-use some of it for holdout.\n",
    "\n",
    "There's no simple threshold for what constitutes a large vs. small dataset. But if your model takes a couple minutes or less to run, it's probably worth switching to cross-validation.\n",
    "\n",
    "Alternatively, you can run cross-validation and see if the scores for each experiment seem close. If each experiment yields the same results, a single validation set is probably sufficient.\n",
    "\n",
    "We obtain the cross-validation scores with the **cross_val_score()** function from scikit-learn. We set the number of folds with the **cv** parameter.\n",
    "\n",
    "The **scoring** parameter chooses a measure of model quality to report: in this case, we chose negative mean absolute error (MAE). The docs for scikit-learn show a [list of options](http://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "\n",
    "It is a little surprising that we specify negative MAE. Scikit-learn has a convention where all metrics are defined so a high number is better. Using negatives here allows them to be consistent with that convention, though negative MAE is almost unheard of elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE scores:\n",
      " [17904.21609589 17471.30986301 17837.17907534 16248.51986301\n",
      " 19082.67856164]\n"
     ]
    }
   ],
   "source": [
    "scores = -1 * cross_val_score(pipeline, x, y , \n",
    "                            cv=5,\n",
    "                            scoring='neg_mean_absolute_error')\n",
    "print(\"MAE scores:\\n\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically want a single measure of model quality to compare alternative models. So we take the average across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE score (across experiments):\n",
      "17708.780691780827\n"
     ]
    }
   ],
   "source": [
    "print(\"Average MAE score (across experiments):\")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cross-validation yields a much better measure of model quality, with the added benefit of cleaning up our code: note that we no longer need to keep track of separate training and validation sets. So, especially for small datasets, it's a good improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using cross-validation to select parameters for a machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns the average MAE over 3 CV folds of random forest model\n",
    "def get_score(n_estimators):\n",
    "    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n",
    "    \n",
    "    Keyword argument:\n",
    "    n_estimators -- the number of trees in the forest\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(n_estimators=n_estimators, random_state=0))\n",
    "    ])\n",
    "    \n",
    "    scores = -1 * cross_val_score(pipeline, X, y,\n",
    "                             cv=3,\n",
    "                             scoring='neg_mean_absolute_error')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test different parameter (n_estimators) values for RandomForest Model\n",
    "You will use the get_score function to evaluate the model performance corresponding to eight different values for the number of trees in the random forest: 50, 100, 150, 200, 250, 300, 350, 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{50: 18037.702795678026, 100: 18153.72029506539, 150: 18025.294838860205, 200: 18021.552920212212, 250: 18007.176285311092, 300: 18028.128146289113, 350: 18017.64052067854, 400: 18009.021758382554}\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [50, 100, 150, 200,250, 300, 350, 400]\n",
    "results = {50 : '', \n",
    "           100 : '', \n",
    "           150 : '', \n",
    "           200 : '', \n",
    "           250 : '', \n",
    "           300 : '', \n",
    "           350 : '', \n",
    "           400 : ''} \n",
    "\n",
    "for estimator in n_estimators: \n",
    "           results[estimator]= get_score(estimator)\n",
    "        \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18007.176285311092\n"
     ]
    }
   ],
   "source": [
    "n_estimators_best = min(results.values())\n",
    "print(n_estimators_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'cross validation evaluation')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize results\n",
    "sns.lineplot(x=list(results.keys()), y=list(results.values()))\n",
    "plt.title('Random Forest cross validation evaluation')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('cross validation evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you have explored one method for choosing appropriate parameters in a machine learning model.  \n",
    "\n",
    "Learn more about [hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization), you're encouraged to start with **grid search**, which is a straightforward method for determining the best _combination_ of parameters for a machine learning model.  Thankfully, scikit-learn also contains a built-in function [`GridSearchCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) that can make your grid search code very efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
